{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7fed56",
   "metadata": {},
   "source": [
    "Reference\n",
    "https://python.langchain.com/docs/tutorials/rag/    \n",
    "\n",
    "#### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a06c52",
   "metadata": {},
   "source": [
    "1. VS Code > Kernel > Install Python + Jupyter   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90afe7a2",
   "metadata": {},
   "source": [
    "2. Create a virtual environment for Python "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ec72758",
   "metadata": {},
   "source": [
    "python3 -m venv .venv    \n",
    ". .venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67588344",
   "metadata": {},
   "source": [
    "3. Install tools    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ee3f3d4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "pip install ipykernel jupyter pandas matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "227ab730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.venv/lib/python3.9/site-packages (21.2.4)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.4\n",
      "    Uninstalling pip-21.2.4:\n",
      "      Successfully uninstalled pip-21.2.4\n",
      "Successfully installed pip-25.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9e44d",
   "metadata": {},
   "source": [
    "#### Install LangChain & Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2e9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4a6b85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU \"langchain[openai]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e87882",
   "metadata": {},
   "source": [
    "##### Store an open key - env\n",
    "https://abc-notes.data.tech.gov.sg/notes/topic-6-ai-agents-with-tools/2.-a-more-secure-way-to-store-credentials.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c4e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a12188f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb78fa8",
   "metadata": {},
   "source": [
    "##### Vector DB - Elasticsearch    \n",
    "\n",
    "* Tutorial : https://python.langchain.com/docs/integrations/vectorstores/elasticsearch/    \n",
    "\n",
    "* Blog : https://www.elastic.co/search-labs/blog/dataset-translation-langchain-python-elastic#:~:text=Loading%20the%20translated%20articles%20into%20a%20vector%20database%20and%20searching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ada6ff",
   "metadata": {},
   "source": [
    "##### Elastic Cloud\n",
    "\n",
    "* Web : https://cloud.elastic.co\n",
    "\n",
    "* Free-trial : https://cloud.elastic.co/registration?utm_source=langchain&utm_content=documentation    \n",
    "\n",
    "* Get started with Elasticsearch Serverless : \n",
    "https://www.elastic.co/docs/solutions/search/serverless-elasticsearch-get-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca2764b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-elasticsearch in ./.venv/lib/python3.9/site-packages (0.3.2)\n",
      "Requirement already satisfied: elasticsearch<9.0.0,>=8.13.1 in ./.venv/lib/python3.9/site-packages (from elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (8.18.1)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in ./.venv/lib/python3.9/site-packages (from langchain-elasticsearch) (0.3.61)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.15.1 in ./.venv/lib/python3.9/site-packages (from elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (8.17.1)\n",
      "Requirement already satisfied: python-dateutil in ./.venv/lib/python3.9/site-packages (from elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.9/site-packages (from elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (4.13.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in ./.venv/lib/python3.9/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (2.4.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.9/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (2025.4.26)\n",
      "Requirement already satisfied: numpy>=1 in ./.venv/lib/python3.9/site-packages (from elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (2.0.2)\n",
      "Requirement already satisfied: simsimd>=3 in ./.venv/lib/python3.9/site-packages (from elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (6.2.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.126 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (0.3.42)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (2.11.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (3.10.18)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (0.23.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.9/site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.9/site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.9/site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (3.4.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.0->langchain-elasticsearch) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil->elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf722500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pdf2image -q\n",
    "%pip install pdfminer -q\n",
    "%pip install pdfminer.six -q\n",
    "%pip install openai -q\n",
    "%pip install scikit-learn -q\n",
    "%pip install rich -q\n",
    "%pip install tqdm -q\n",
    "%pip install pandas -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c418c3",
   "metadata": {},
   "source": [
    "# Install poppler in the terminal\n",
    "brew install poppler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f125cefa",
   "metadata": {},
   "source": [
    "#### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e66050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pdf2image import convert_from_path\n",
    "from pdf2image.exceptions import (\n",
    "    PDFInfoNotInstalledError,\n",
    "    PDFPageCountError,\n",
    "    PDFSyntaxError\n",
    ")\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import re \n",
    "import numpy as np\n",
    "from rich import print\n",
    "\n",
    "def convert_doc_to_images(path):\n",
    "    images = convert_from_path(path)\n",
    "    return images\n",
    "\n",
    "def extract_text_from_doc(path):\n",
    "    text = extract_text(path)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14939cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pillow -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be2ea04",
   "metadata": {},
   "source": [
    "#### Convert pdf to image\n",
    "https://cookbook.openai.com/examples/parse_pdf_docs_for_rag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db17233",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.abspath(\"data.pdf\")\n",
    "imgs = convert_doc_to_images(file_path)\n",
    "for img in imgs:\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "from langchain.schema import Document\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3a4f54ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f418785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting images to base64 encoded images\n",
    "def get_img_uri(img):\n",
    "    png_buffer = io.BytesIO()\n",
    "    img.save(png_buffer, format=\"PNG\")\n",
    "    png_buffer.seek(0)\n",
    "\n",
    "    base64_png = base64.b64encode(png_buffer.read()).decode('utf-8')\n",
    "\n",
    "    data_uri = f\"data:image/png;base64,{base64_png}\"\n",
    "    return data_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538e6a87",
   "metadata": {},
   "source": [
    "#### Extract text from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "541e7ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF - 설명글 / 테이블 / 다이어그램 / 코드 / 참고문헌\n",
    "\n",
    "extract_prompt = '''\n",
    "You will be provided with images containing text, representing multiple pages of a document.\n",
    "\n",
    "Your task is to extract all readable text from the images **as accurately and completely as possible, and organize it by logical sections.\n",
    "\n",
    "- **Do Not** add any explanations, summaries, or interpretations.\n",
    "- **Do Not** mark or mention page numbers in the output.\n",
    "- Remove any isolated date stamps, page numbers, headers, footer, or other non-content elements.\n",
    "- Exclude any repeated headers titled \"Prompt Engineering\"\n",
    "- If a subheading is clearly present (e.g., bolded, underlined, capitalized, or formatted distinctly), start a new section from that point.\n",
    "- Maintain the original order, flow of the content, and line breaks.\n",
    "\n",
    "- **Diagrams**: Explain each component and how they interact. For example, \"The process begins with X, which then leads to Y and results in Z.\"\n",
    "- **Tables**: Break down the information logically. In the table, bolded characters represent the type or name of the data (such as variable names or categories), while non-bolded characters represent the actual data values. For example, \"Product A costs 100 dollars, while Product B is priced at 200 dollars.\"\n",
    "\n",
    "Output should be structured as a sequence of content sections divided by meaningful subheadings or topic shifts.\n",
    "\n",
    "------\n",
    "\n",
    "If there is an identifiable title, present the output in the following format:\n",
    "\n",
    "{TITLE}\n",
    "{Content description}\n",
    "'''\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def analyze_image(data_uri):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": extract_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"{data_uri}\"}\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0,\n",
    "        top_p=0.1\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1bcbaae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 정리\n",
    "def clean_text(text):\n",
    "    content = text.replace(' \\n', '').replace('\\n\\n', '\\n').replace('\\n\\n\\n', '\\n').strip()\n",
    "    content = re.sub(r\"\\*{1,2}\", \"\", content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "660c5102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 추출\n",
    "documents = []\n",
    "for i, img in enumerate(imgs):\n",
    "    data_uri = get_img_uri(img)  # img 객체를 data_uri로 변환\n",
    "    text = analyze_image(data_uri)\n",
    "    text = clean_text(text)\n",
    "    doc = Document(page_content=text, metadata={\"image_name\": f\"page_{i+1}\"})\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c083b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40430072",
   "metadata": {},
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b261ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'page_12'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'System, contextual and role prompting\\nSystem, contextual and role prompting are all techniques </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">used to guide how LLMs generate text, but they focus on different aspects:\\n- System prompting sets the overall </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">context and purpose for the language model. It defines the ‘big picture’ of what the model should be doing, like </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">translating a language, classifying a review etc.\\n- Contextual prompting provides specific details or background </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information relevant to the current conversation or task. It helps the model to understand the nuances of what’s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">being asked and tailor the response accordingly.\\n- Role prompting assigns a specific character or identity for the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language model to adopt. This helps the model generate responses that are consistent with the assigned role and its</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">associated knowledge and behavior.\\nThere can be considerable overlap between system, contextual, and role </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompting. E.g. a prompt that assigns a role to the system, can also have a context.\\nHowever, each type of prompt </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">serves a slightly different primary purpose:\\n- System prompt: Defines the model’s fundamental capabilities and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">overarching purpose.\\n- Contextual prompt: Provides immediate, task-specific information to guide the response. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">It’s highly specific to the current task or input, which is dynamic.\\n- Role prompt: Frames the model’s output </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">style and voice. It adds a layer of specificity and personality.'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'image_name'\u001b[0m: \u001b[32m'page_12'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'System, contextual and role prompting\\nSystem, contextual and role prompting are all techniques \u001b[0m\n",
       "\u001b[32mused to guide how LLMs generate text, but they focus on different aspects:\\n- System prompting sets the overall \u001b[0m\n",
       "\u001b[32mcontext and purpose for the language model. It defines the ‘big picture’ of what the model should be doing, like \u001b[0m\n",
       "\u001b[32mtranslating a language, classifying a review etc.\\n- Contextual prompting provides specific details or background \u001b[0m\n",
       "\u001b[32minformation relevant to the current conversation or task. It helps the model to understand the nuances of what’s \u001b[0m\n",
       "\u001b[32mbeing asked and tailor the response accordingly.\\n- Role prompting assigns a specific character or identity for the\u001b[0m\n",
       "\u001b[32mlanguage model to adopt. This helps the model generate responses that are consistent with the assigned role and its\u001b[0m\n",
       "\u001b[32massociated knowledge and behavior.\\nThere can be considerable overlap between system, contextual, and role \u001b[0m\n",
       "\u001b[32mprompting. E.g. a prompt that assigns a role to the system, can also have a context.\\nHowever, each type of prompt \u001b[0m\n",
       "\u001b[32mserves a slightly different primary purpose:\\n- System prompt: Defines the model’s fundamental capabilities and \u001b[0m\n",
       "\u001b[32moverarching purpose.\\n- Contextual prompt: Provides immediate, task-specific information to guide the response. \u001b[0m\n",
       "\u001b[32mIt’s highly specific to the current task or input, which is dynamic.\\n- Role prompt: Frames the model’s output \u001b[0m\n",
       "\u001b[32mstyle and voice. It adds a layer of specificity and personality.'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'page_12'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'System, contextual and role prompting\\nSystem, contextual and role prompting are all techniques </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">used to guide how LLMs generate text, but they focus on different aspects:\\n System prompting sets the overall </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">context and purpose for the language model. It defines the ‘big picture’ of what the model should be doing, like </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">translating a language, classifying a review etc.\\n Contextual prompting provides specific details or background </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information relevant to the current conversation or task. It helps the model to understand the nuances of what’s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">being asked and tailor the response accordingly.\\n Role prompting assigns a specific character or identity for the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language model to adopt. This helps the model generate responses that are consistent with the assigned role and its</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">associated knowledge and behavior.\\nThere can be considerable overlap between system, contextual, and role </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompting. E.g. a prompt that assigns a role to the system, can also have a context.\\nHowever, each type of prompt </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">serves a slightly different primary purpose:\\n System prompt: Defines the model’s fundamental capabilities and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">overarching purpose.\\n Contextual prompt: Provides immediate, taskspecific information to guide the response. It’s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">highly specific to the current task or input, which is dynamic.\\n Role prompt: Frames the model’s output style and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">voice. It adds a layer of specificity and personality.'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'image_name'\u001b[0m: \u001b[32m'page_12'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'System, contextual and role prompting\\nSystem, contextual and role prompting are all techniques \u001b[0m\n",
       "\u001b[32mused to guide how LLMs generate text, but they focus on different aspects:\\n System prompting sets the overall \u001b[0m\n",
       "\u001b[32mcontext and purpose for the language model. It defines the ‘big picture’ of what the model should be doing, like \u001b[0m\n",
       "\u001b[32mtranslating a language, classifying a review etc.\\n Contextual prompting provides specific details or background \u001b[0m\n",
       "\u001b[32minformation relevant to the current conversation or task. It helps the model to understand the nuances of what’s \u001b[0m\n",
       "\u001b[32mbeing asked and tailor the response accordingly.\\n Role prompting assigns a specific character or identity for the \u001b[0m\n",
       "\u001b[32mlanguage model to adopt. This helps the model generate responses that are consistent with the assigned role and its\u001b[0m\n",
       "\u001b[32massociated knowledge and behavior.\\nThere can be considerable overlap between system, contextual, and role \u001b[0m\n",
       "\u001b[32mprompting. E.g. a prompt that assigns a role to the system, can also have a context.\\nHowever, each type of prompt \u001b[0m\n",
       "\u001b[32mserves a slightly different primary purpose:\\n System prompt: Defines the model’s fundamental capabilities and \u001b[0m\n",
       "\u001b[32moverarching purpose.\\n Contextual prompt: Provides immediate, taskspecific information to guide the response. It’s \u001b[0m\n",
       "\u001b[32mhighly specific to the current task or input, which is dynamic.\\n Role prompt: Frames the model’s output style and \u001b[0m\n",
       "\u001b[32mvoice. It adds a layer of specificity and personality.'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = documents[11]\n",
    "print(doc)\n",
    "\n",
    "new_content = doc.page_content.replace('-', '')\n",
    "new = Document(metadata=doc.metadata, page_content=new_content)\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854b6f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[11] = new\n",
    "print(documents[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "28d77967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'page_13'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Distinguishing between system, contextual, and role prompts provides a framework for designing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompts with clear intent, allowing for flexible combinations and making it easier to analyze how each prompt type </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">influences the language model’s output.\\nLet’s dive into these three different kinds of prompts.\\nSystem </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompting\\nTable 3 contains a system prompt, where I specify additional information on how to return the output. I </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increased the temperature to get a higher creativity level, and I specified a higher token limit. However, because </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of my clear instruction on how to return the output the model didn’t return extra text.\\nTable 3. An example of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">system prompting\\nGoal: Classify movie reviews as positive, neutral or negative.\\nModel: gemini-pro\\nTemperature: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1\\nToken Limit: 5\\nTop-K: 40\\nTop-P: 0.8\\nPrompt:  Classify movie reviews as positive, neutral or negative. Only </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">return the label in uppercase.\\n  Review: \"Her\" is a disturbing study revealing the direction humanity is headed if</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">AI is allowed to keep evolving, unchecked. It\\'s so disturbing I couldn\\'t watch it.\\n  Sentiment:\\nOutput: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">NEGATIVE'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'image_name'\u001b[0m: \u001b[32m'page_13'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'Distinguishing between system, contextual, and role prompts provides a framework for designing \u001b[0m\n",
       "\u001b[32mprompts with clear intent, allowing for flexible combinations and making it easier to analyze how each prompt type \u001b[0m\n",
       "\u001b[32minfluences the language model’s output.\\nLet’s dive into these three different kinds of prompts.\\nSystem \u001b[0m\n",
       "\u001b[32mprompting\\nTable 3 contains a system prompt, where I specify additional information on how to return the output. I \u001b[0m\n",
       "\u001b[32mincreased the temperature to get a higher creativity level, and I specified a higher token limit. However, because \u001b[0m\n",
       "\u001b[32mof my clear instruction on how to return the output the model didn’t return extra text.\\nTable 3. An example of \u001b[0m\n",
       "\u001b[32msystem prompting\\nGoal: Classify movie reviews as positive, neutral or negative.\\nModel: gemini-pro\\nTemperature: \u001b[0m\n",
       "\u001b[32m1\\nToken Limit: 5\\nTop-K: 40\\nTop-P: 0.8\\nPrompt:  Classify movie reviews as positive, neutral or negative. Only \u001b[0m\n",
       "\u001b[32mreturn the label in uppercase.\\n  Review: \"Her\" is a disturbing study revealing the direction humanity is headed if\u001b[0m\n",
       "\u001b[32mAI is allowed to keep evolving, unchecked. It\\'s so disturbing I couldn\\'t watch it.\\n  Sentiment:\\nOutput: \u001b[0m\n",
       "\u001b[32mNEGATIVE'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'page_13'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Distinguishing between system, contextual, and role prompts provides a framework for designing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompts with clear intent, allowing for flexible combinations and making it easier to analyze how each prompt type </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">influences the language model’s output.\\nLet’s dive into these three different kinds of prompts.\\nSystem </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompting\\nTable 3 contains a system prompt, where I specify additional information on how to return the output. I </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">increased the temperature to get a higher creativity level, and I specified a higher token limit. However, because </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of my clear instruction on how to return the output the model didn’t return extra text.\\nTable 3. An example of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">system prompting\\nGoal: Classify movie reviews as positive, neutral or negative.\\nModel: geminipro\\nTemperature: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1\\nToken Limit: 5\\nTopK: 40\\nTopP: 0.8\\nPrompt: Classify movie reviews as positive, neutral or negative. Only </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">return the label in uppercase. Review: \"Her\" is a disturbing study revealing the direction humanity is headed if AI</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is allowed to keep evolving, unchecked. It\\'s so disturbing I couldn\\'t watch it. Sentiment:\\nOutput: NEGATIVE'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'image_name'\u001b[0m: \u001b[32m'page_13'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'Distinguishing between system, contextual, and role prompts provides a framework for designing \u001b[0m\n",
       "\u001b[32mprompts with clear intent, allowing for flexible combinations and making it easier to analyze how each prompt type \u001b[0m\n",
       "\u001b[32minfluences the language model’s output.\\nLet’s dive into these three different kinds of prompts.\\nSystem \u001b[0m\n",
       "\u001b[32mprompting\\nTable 3 contains a system prompt, where I specify additional information on how to return the output. I \u001b[0m\n",
       "\u001b[32mincreased the temperature to get a higher creativity level, and I specified a higher token limit. However, because \u001b[0m\n",
       "\u001b[32mof my clear instruction on how to return the output the model didn’t return extra text.\\nTable 3. An example of \u001b[0m\n",
       "\u001b[32msystem prompting\\nGoal: Classify movie reviews as positive, neutral or negative.\\nModel: geminipro\\nTemperature: \u001b[0m\n",
       "\u001b[32m1\\nToken Limit: 5\\nTopK: 40\\nTopP: 0.8\\nPrompt: Classify movie reviews as positive, neutral or negative. Only \u001b[0m\n",
       "\u001b[32mreturn the label in uppercase. Review: \"Her\" is a disturbing study revealing the direction humanity is headed if AI\u001b[0m\n",
       "\u001b[32mis allowed to keep evolving, unchecked. It\\'s so disturbing I couldn\\'t watch it. Sentiment:\\nOutput: NEGATIVE'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = documents[12]\n",
    "print(doc)\n",
    "\n",
    "new_content = re.sub(r'\\s{2,}', ' ', doc.page_content.replace('-', ''))\n",
    "new = Document(metadata=doc.metadata, page_content=new_content)\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed587d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[12] = new\n",
    "print(documents[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad98f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'page_15'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Classify Movie Reviews as Positive, Neutral, or Negative, Return JSON\\nGoal Classify movie </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reviews as positive, neutral, or negative, return JSON.\\nModel gemini-pro\\nTemperature 1\\nToken Limit 1024\\nTop-K </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">40\\nTop-P 0.8\\nPrompt Classify movie reviews as positive, neutral or negative. Return valid JSON:\\nReview: \"Her\" is</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">so disturbing I couldn\\'t watch it.\\nSchema:\\n```\\nMOVIE:\\n{\\n  \"sentiment\": String \"POSITIVE\" | \"NEGATIVE\" | </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"NEUTRAL\",\\n  \"name\": String\\n}\\nMOVIE REVIEWS:\\n{\\n  \"movie_reviews\": [MOVIE]\\n}\\n```\\nJSON </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Response:\\nOutput\\n```\\n{\\n  \"movie_reviews\": [\\n    {\\n      \"sentiment\": \"NEGATIVE\",\\n      \"name\": \"Her\"\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">}\\n  ]\\n}\\n```\\nTable 4. An example of system prompting with JSON format'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'image_name'\u001b[0m: \u001b[32m'page_15'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'Classify Movie Reviews as Positive, Neutral, or Negative, Return JSON\\nGoal Classify movie \u001b[0m\n",
       "\u001b[32mreviews as positive, neutral, or negative, return JSON.\\nModel gemini-pro\\nTemperature 1\\nToken Limit 1024\\nTop-K \u001b[0m\n",
       "\u001b[32m40\\nTop-P 0.8\\nPrompt Classify movie reviews as positive, neutral or negative. Return valid JSON:\\nReview: \"Her\" is\u001b[0m\n",
       "\u001b[32ma disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It\\'s \u001b[0m\n",
       "\u001b[32mso disturbing I couldn\\'t watch it.\\nSchema:\\n```\\nMOVIE:\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n  \"sentiment\": String \"POSITIVE\" | \"NEGATIVE\" | \u001b[0m\n",
       "\u001b[32m\"NEUTRAL\",\\n  \"name\": String\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\nMOVIE REVIEWS:\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n  \"movie_reviews\": \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMOVIE\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n```\\nJSON \u001b[0m\n",
       "\u001b[32mResponse:\\nOutput\\n```\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n  \"movie_reviews\": \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\n    \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n      \"sentiment\": \"NEGATIVE\",\\n      \"name\": \"Her\"\\n    \u001b[0m\n",
       "\u001b[32m}\u001b[0m\u001b[32m\\n  \u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n```\\nTable 4. An example of system prompting with JSON format'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'page_15'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Classify Movie Reviews as Positive, Neutral, or Negative, Return JSON\\nGoal Classify movie </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reviews as positive, neutral, or negative, return JSON.\\nModel gemini-pro\\nTemperature 1\\nToken Limit 1024\\nTop-K </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">40\\nTop-P 0.8\\nPrompt Classify movie reviews as positive, neutral or negative. Return valid JSON:\\nReview: \"Her\" is</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">so disturbing I couldn\\'t watch it.\\nSchema: MOVIE:\\n{ \"sentiment\": String \"POSITIVE\" | \"NEGATIVE\" | \"NEUTRAL\", </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"name\": String\\n}\\nMOVIE REVIEWS:\\n{ \"movie_reviews\": [MOVIE]\\n} JSON Response:\\nOutput { \"movie_reviews\": [ { </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"sentiment\": \"NEGATIVE\", \"name\": \"Her\" } ]\\n} Table 4. An example of system prompting with JSON format'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'image_name'\u001b[0m: \u001b[32m'page_15'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'Classify Movie Reviews as Positive, Neutral, or Negative, Return JSON\\nGoal Classify movie \u001b[0m\n",
       "\u001b[32mreviews as positive, neutral, or negative, return JSON.\\nModel gemini-pro\\nTemperature 1\\nToken Limit 1024\\nTop-K \u001b[0m\n",
       "\u001b[32m40\\nTop-P 0.8\\nPrompt Classify movie reviews as positive, neutral or negative. Return valid JSON:\\nReview: \"Her\" is\u001b[0m\n",
       "\u001b[32ma disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving, unchecked. It\\'s \u001b[0m\n",
       "\u001b[32mso disturbing I couldn\\'t watch it.\\nSchema: MOVIE:\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32m \"sentiment\": String \"POSITIVE\" | \"NEGATIVE\" | \"NEUTRAL\", \u001b[0m\n",
       "\u001b[32m\"name\": String\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\nMOVIE REVIEWS:\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32m \"movie_reviews\": \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMOVIE\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m JSON Response:\\nOutput \u001b[0m\u001b[32m{\u001b[0m\u001b[32m \"movie_reviews\": \u001b[0m\u001b[32m[\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m\"sentiment\": \"NEGATIVE\", \"name\": \"Her\" \u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m Table 4. An example of system prompting with JSON format'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = documents[14]\n",
    "print(doc)\n",
    "\n",
    "new_content = re.sub(r'\\s{2,}', ' ', doc.page_content.replace('```', ''))\n",
    "new = Document(metadata=doc.metadata, page_content=new_content)\n",
    "\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[14] = new\n",
    "print(documents[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "92a65970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'page_19'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Suggesting Articles for a Blog About Retro Games\\nGoal: Suggest articles for a blog about retro </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">games\\nModel: gemini-pro\\nTemperature: 1\\nToken Limit: 1024\\nTop-K: 40\\nTop-P: 0.8\\nPrompt:Context: You are writing</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for a blog about retro 80's arcade video games. Suggest 3 topics to write an article about with a few lines of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">description of what this article should contain.\\nOutput:\\n1. The Evolution of Arcade Cabinet Design:\\n   - This </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">article would explore the evolution of arcade cabinet designs, from the early wood and metal cabinets of the 1970s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to the sleek, neon-lit designs of the 1980s and beyond.\\n2. Blast From The Past: Iconic Arcade Games of The 80’s:\\n</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">- This article would feature a glimpse into the history, impact, and legacy of some of the most iconic and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">influential arcade games of the 1980s, highlighting their innovations, popular mechanics, and enduring charm.\\n3. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">The Rise and Retro Revival of Pixel Art:\\n   - This article would delve into the evolution of pixel art as a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">defining visual style of early arcade games, tracing its roots, exploring its techniques, and discussing the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">resurgence of pixel art in modern games and digital art.\"</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'image_name'\u001b[0m: \u001b[32m'page_19'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m\"Suggesting\u001b[0m\u001b[32m Articles for a Blog About Retro Games\\nGoal: Suggest articles for a blog about retro \u001b[0m\n",
       "\u001b[32mgames\\nModel: gemini-pro\\nTemperature: 1\\nToken Limit: 1024\\nTop-K: 40\\nTop-P: 0.8\\nPrompt:Context: You are writing\u001b[0m\n",
       "\u001b[32mfor a blog about retro 80's arcade video games. Suggest 3 topics to write an article about with a few lines of \u001b[0m\n",
       "\u001b[32mdescription of what this article should contain.\\nOutput:\\n1. The Evolution of Arcade Cabinet Design:\\n   - This \u001b[0m\n",
       "\u001b[32marticle would explore the evolution of arcade cabinet designs, from the early wood and metal cabinets of the 1970s \u001b[0m\n",
       "\u001b[32mto the sleek, neon-lit designs of the 1980s and beyond.\\n2. Blast From The Past: Iconic Arcade Games of The 80’s:\\n\u001b[0m\n",
       "\u001b[32m- This article would feature a glimpse into the history, impact, and legacy of some of the most iconic and \u001b[0m\n",
       "\u001b[32minfluential arcade games of the 1980s, highlighting their innovations, popular mechanics, and enduring charm.\\n3. \u001b[0m\n",
       "\u001b[32mThe Rise and Retro Revival of Pixel Art:\\n   - This article would delve into the evolution of pixel art as a \u001b[0m\n",
       "\u001b[32mdefining visual style of early arcade games, tracing its roots, exploring its techniques, and discussing the \u001b[0m\n",
       "\u001b[32mresurgence of pixel art in modern games and digital art.\"\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'page_19'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Suggesting Articles for a Blog About Retro Games\\nGoal: Suggest articles for a blog about retro </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">games\\nModel: geminipro\\nTemperature: 1\\nToken Limit: 1024\\nTopK: 40\\nTopP: 0.8\\nPrompt:Context: You are writing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for a blog about retro 80's arcade video games. Suggest 3 topics to write an article about with a few lines of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">description of what this article should contain.\\nOutput:\\n1. The Evolution of Arcade Cabinet Design: This article </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">would explore the evolution of arcade cabinet designs, from the early wood and metal cabinets of the 1970s to the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sleek, neonlit designs of the 1980s and beyond.\\n2. Blast From The Past: Iconic Arcade Games of The 80’s: This </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">article would feature a glimpse into the history, impact, and legacy of some of the most iconic and influential </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">arcade games of the 1980s, highlighting their innovations, popular mechanics, and enduring charm.\\n3. The Rise and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Retro Revival of Pixel Art: This article would delve into the evolution of pixel art as a defining visual style of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">early arcade games, tracing its roots, exploring its techniques, and discussing the resurgence of pixel art in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">modern games and digital art.\"</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'image_name'\u001b[0m: \u001b[32m'page_19'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m\"Suggesting\u001b[0m\u001b[32m Articles for a Blog About Retro Games\\nGoal: Suggest articles for a blog about retro \u001b[0m\n",
       "\u001b[32mgames\\nModel: geminipro\\nTemperature: 1\\nToken Limit: 1024\\nTopK: 40\\nTopP: 0.8\\nPrompt:Context: You are writing \u001b[0m\n",
       "\u001b[32mfor a blog about retro 80's arcade video games. Suggest 3 topics to write an article about with a few lines of \u001b[0m\n",
       "\u001b[32mdescription of what this article should contain.\\nOutput:\\n1. The Evolution of Arcade Cabinet Design: This article \u001b[0m\n",
       "\u001b[32mwould explore the evolution of arcade cabinet designs, from the early wood and metal cabinets of the 1970s to the \u001b[0m\n",
       "\u001b[32msleek, neonlit designs of the 1980s and beyond.\\n2. Blast From The Past: Iconic Arcade Games of The 80’s: This \u001b[0m\n",
       "\u001b[32marticle would feature a glimpse into the history, impact, and legacy of some of the most iconic and influential \u001b[0m\n",
       "\u001b[32marcade games of the 1980s, highlighting their innovations, popular mechanics, and enduring charm.\\n3. The Rise and \u001b[0m\n",
       "\u001b[32mRetro Revival of Pixel Art: This article would delve into the evolution of pixel art as a defining visual style of \u001b[0m\n",
       "\u001b[32mearly arcade games, tracing its roots, exploring its techniques, and discussing the resurgence of pixel art in \u001b[0m\n",
       "\u001b[32mmodern games and digital art.\"\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = documents[18]\n",
    "print(doc)\n",
    "\n",
    "new_content = re.sub(r'\\s{2,}', ' ', doc.page_content.replace('-', ''))\n",
    "new = Document(metadata=doc.metadata, page_content=new_content)\n",
    "\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60781ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[18] = new\n",
    "print(documents[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4f6e5362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'page_28'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"EMAIL:\\nHi,\\nI have seen you use Wordpress for your website. A great open source content </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">management system. I have used it in the past too. It comes with lots of great user plugins. And it's pretty easy </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to set up.\\nI did notice a bug in the contact form, which happens when you select the name field. See the attached </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">screenshot of me entering text in the name field. Notice the JavaScript alert box that I inv0k3d.\\nBut for the rest</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">it's a great website. I enjoy reading it. Feel free to leave the bug in the website, because it gives me more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interesting things to read.\\nCheers,\\nHarry the Hacker.\\n---\\nClassify the above email as IMPORTANT or NOT </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">IMPORTANT. Let's think step by step and explain why.\"</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'image_name'\u001b[0m: \u001b[32m'page_28'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m\"EMAIL\u001b[0m\u001b[32m:\\nHi,\\nI have seen you use Wordpress for your website. A great open source content \u001b[0m\n",
       "\u001b[32mmanagement system. I have used it in the past too. It comes with lots of great user plugins. And it's pretty easy \u001b[0m\n",
       "\u001b[32mto set up.\\nI did notice a bug in the contact form, which happens when you select the name field. See the attached \u001b[0m\n",
       "\u001b[32mscreenshot of me entering text in the name field. Notice the JavaScript alert box that I inv0k3d.\\nBut for the rest\u001b[0m\n",
       "\u001b[32mit's a great website. I enjoy reading it. Feel free to leave the bug in the website, because it gives me more \u001b[0m\n",
       "\u001b[32minteresting things to read.\\nCheers,\\nHarry the Hacker.\\n---\\nClassify the above email as IMPORTANT or NOT \u001b[0m\n",
       "\u001b[32mIMPORTANT. Let's think step by step and explain why.\"\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'page_28'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"EMAIL:\\nHi,\\nI have seen you use Wordpress for your website. A great open source content </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">management system. I have used it in the past too. It comes with lots of great user plugins. And it's pretty easy </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to set up.\\nI did notice a bug in the contact form, which happens when you select the name field. See the attached </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">screenshot of me entering text in the name field. Notice the JavaScript alert box that I inv0k3d.\\nBut for the rest</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">it's a great website. I enjoy reading it. Feel free to leave the bug in the website, because it gives me more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interesting things to read.\\nCheers,\\nHarry the Hacker. Classify the above email as IMPORTANT or NOT IMPORTANT. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Let's think step by step and explain why.\"</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'image_name'\u001b[0m: \u001b[32m'page_28'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m\"EMAIL\u001b[0m\u001b[32m:\\nHi,\\nI have seen you use Wordpress for your website. A great open source content \u001b[0m\n",
       "\u001b[32mmanagement system. I have used it in the past too. It comes with lots of great user plugins. And it's pretty easy \u001b[0m\n",
       "\u001b[32mto set up.\\nI did notice a bug in the contact form, which happens when you select the name field. See the attached \u001b[0m\n",
       "\u001b[32mscreenshot of me entering text in the name field. Notice the JavaScript alert box that I inv0k3d.\\nBut for the rest\u001b[0m\n",
       "\u001b[32mit's a great website. I enjoy reading it. Feel free to leave the bug in the website, because it gives me more \u001b[0m\n",
       "\u001b[32minteresting things to read.\\nCheers,\\nHarry the Hacker. Classify the above email as IMPORTANT or NOT IMPORTANT. \u001b[0m\n",
       "\u001b[32mLet's think step by step and explain why.\"\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = documents[27]\n",
    "print(doc)\n",
    "\n",
    "new_content = re.sub(r'\\s{2,}', ' ', doc.page_content.replace('---', ''))\n",
    "new = Document(metadata=doc.metadata, page_content=new_content)\n",
    "\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c3035",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[27] = new\n",
    "print(documents[27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5458f242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'page_58'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Once you feel the prompt is close to perfect, take it to your project codebase. And in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">codebase, save prompts in a separate file from code, so it’s easier to maintain. Finally, ideally your prompts are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">part of an operationalized system, and as a prompt engineer you should rely on automated tests and evaluation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">procedures to understand how well your prompt generalizes to a task.\\nPrompt engineering is an iterative process. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Craft and test different prompts, analyze, and document the results. Refine your prompt based on the model’s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance. Keep experimenting until you achieve the desired output. When you change a model or model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">configuration, go back and keep experimenting with the previously used prompts.\\nTable 21. A template for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documenting prompts\\n| Name          | [name and version of your prompt]            </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n|-------------------|----------------------------------------------|\\n| Goal          | [One sentence </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">explanation of the goal of this attempt] |\\n| Model         | [name and version of the used model]         |\\n| </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Temperature   | [value between 0 - 1]                        |\\n| Token Limit   | [number]                         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| Top-K         | [number]                                     |\\n| Top-P         | [number]                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| Prompt        | [Write all the full prompt]                  |\\n| Output        | [Write out the output or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multiple outputs]   |\\nSummary\\nThis whitepaper discusses prompt engineering. We learned various prompting </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">techniques, such as:\\n- Zero prompting\\n- Few shot prompting'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'image_name'\u001b[0m: \u001b[32m'page_58'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'Once you feel the prompt is close to perfect, take it to your project codebase. And in the \u001b[0m\n",
       "\u001b[32mcodebase, save prompts in a separate file from code, so it’s easier to maintain. Finally, ideally your prompts are \u001b[0m\n",
       "\u001b[32mpart of an operationalized system, and as a prompt engineer you should rely on automated tests and evaluation \u001b[0m\n",
       "\u001b[32mprocedures to understand how well your prompt generalizes to a task.\\nPrompt engineering is an iterative process. \u001b[0m\n",
       "\u001b[32mCraft and test different prompts, analyze, and document the results. Refine your prompt based on the model’s \u001b[0m\n",
       "\u001b[32mperformance. Keep experimenting until you achieve the desired output. When you change a model or model \u001b[0m\n",
       "\u001b[32mconfiguration, go back and keep experimenting with the previously used prompts.\\nTable 21. A template for \u001b[0m\n",
       "\u001b[32mdocumenting prompts\\n| Name          | \u001b[0m\u001b[32m[\u001b[0m\u001b[32mname and version of your prompt\u001b[0m\u001b[32m]\u001b[0m\u001b[32m            \u001b[0m\n",
       "\u001b[32m|\\n|-------------------|----------------------------------------------|\\n| Goal          | \u001b[0m\u001b[32m[\u001b[0m\u001b[32mOne sentence \u001b[0m\n",
       "\u001b[32mexplanation of the goal of this attempt\u001b[0m\u001b[32m]\u001b[0m\u001b[32m |\\n| Model         | \u001b[0m\u001b[32m[\u001b[0m\u001b[32mname and version of the used model\u001b[0m\u001b[32m]\u001b[0m\u001b[32m         |\\n| \u001b[0m\n",
       "\u001b[32mTemperature   | \u001b[0m\u001b[32m[\u001b[0m\u001b[32mvalue between 0 - 1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m                        |\\n| Token Limit   | \u001b[0m\u001b[32m[\u001b[0m\u001b[32mnumber\u001b[0m\u001b[32m]\u001b[0m\u001b[32m                         \u001b[0m\n",
       "\u001b[32m|\\n| Top-K         | \u001b[0m\u001b[32m[\u001b[0m\u001b[32mnumber\u001b[0m\u001b[32m]\u001b[0m\u001b[32m                                     |\\n| Top-P         | \u001b[0m\u001b[32m[\u001b[0m\u001b[32mnumber\u001b[0m\u001b[32m]\u001b[0m\u001b[32m                    \u001b[0m\n",
       "\u001b[32m|\\n| Prompt        | \u001b[0m\u001b[32m[\u001b[0m\u001b[32mWrite all the full prompt\u001b[0m\u001b[32m]\u001b[0m\u001b[32m                  |\\n| Output        | \u001b[0m\u001b[32m[\u001b[0m\u001b[32mWrite out the output or \u001b[0m\n",
       "\u001b[32mmultiple outputs\u001b[0m\u001b[32m]\u001b[0m\u001b[32m   |\\nSummary\\nThis whitepaper discusses prompt engineering. We learned various prompting \u001b[0m\n",
       "\u001b[32mtechniques, such as:\\n- Zero prompting\\n- Few shot prompting'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'page_58'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Once you feel the prompt is close to perfect, take it to your project codebase. And in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">codebase, save prompts in a separate file from code, so it’s easier to maintain. Finally, ideally your prompts are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">part of an operationalized system, and as a prompt engineer you should rely on automated tests and evaluation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">procedures to understand how well your prompt generalizes to a task.\\nPrompt engineering is an iterative process. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Craft and test different prompts, analyze, and document the results. Refine your prompt based on the model’s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance. Keep experimenting until you achieve the desired output. When you change a model or model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">configuration, go back and keep experimenting with the previously used prompts.\\nTable 21. A template for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documenting prompts Name [name and version of your prompt] Goal [One sentence explanation of the goal of this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attempt] Model [name and version of the used model] Temperature [value between 0 1] Token Limit [number] Top K </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[number] Top P [number] Prompt [Write all the full prompt] Output [Write out the output or multiple outputs] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Summary\\nThis whitepaper discusses prompt engineering. We learned various prompting techniques, such as: Zero </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompting Few shot prompting'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'image_name'\u001b[0m: \u001b[32m'page_58'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'Once you feel the prompt is close to perfect, take it to your project codebase. And in the \u001b[0m\n",
       "\u001b[32mcodebase, save prompts in a separate file from code, so it’s easier to maintain. Finally, ideally your prompts are \u001b[0m\n",
       "\u001b[32mpart of an operationalized system, and as a prompt engineer you should rely on automated tests and evaluation \u001b[0m\n",
       "\u001b[32mprocedures to understand how well your prompt generalizes to a task.\\nPrompt engineering is an iterative process. \u001b[0m\n",
       "\u001b[32mCraft and test different prompts, analyze, and document the results. Refine your prompt based on the model’s \u001b[0m\n",
       "\u001b[32mperformance. Keep experimenting until you achieve the desired output. When you change a model or model \u001b[0m\n",
       "\u001b[32mconfiguration, go back and keep experimenting with the previously used prompts.\\nTable 21. A template for \u001b[0m\n",
       "\u001b[32mdocumenting prompts Name \u001b[0m\u001b[32m[\u001b[0m\u001b[32mname and version of your prompt\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Goal \u001b[0m\u001b[32m[\u001b[0m\u001b[32mOne sentence explanation of the goal of this \u001b[0m\n",
       "\u001b[32mattempt\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Model \u001b[0m\u001b[32m[\u001b[0m\u001b[32mname and version of the used model\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Temperature \u001b[0m\u001b[32m[\u001b[0m\u001b[32mvalue between 0 1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Token Limit \u001b[0m\u001b[32m[\u001b[0m\u001b[32mnumber\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Top K \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mnumber\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Top P \u001b[0m\u001b[32m[\u001b[0m\u001b[32mnumber\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Prompt \u001b[0m\u001b[32m[\u001b[0m\u001b[32mWrite all the full prompt\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Output \u001b[0m\u001b[32m[\u001b[0m\u001b[32mWrite out the output or multiple outputs\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mSummary\\nThis whitepaper discusses prompt engineering. We learned various prompting techniques, such as: Zero \u001b[0m\n",
       "\u001b[32mprompting Few shot prompting'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = documents[57]\n",
    "print(doc)\n",
    "\n",
    "new_content = re.sub(r'[-|｜]', ' ', doc.page_content)\n",
    "new_content = re.sub(r'\\s{2,}', ' ', new_content)\n",
    "new = Document(metadata=doc.metadata, page_content=new_content)\n",
    "\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac81271",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[57] = new\n",
    "print(documents[57])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f6d99332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'page_59'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Prompt Engineering\\n- System prompting\\n- Role prompting\\n- Contextual prompting\\n- Step-back </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompting\\n- Chain of thought\\n- Self consistency\\n- Tree of thoughts\\n- ReAct\\nWe even looked into ways how you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">can automate your prompts.\\nThe whitepaper then discusses the challenges of gen AI like the problems that can </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">happen when your prompts are insufficient. We closed with best practices on how to become a better prompt </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">engineer.'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'image_name'\u001b[0m: \u001b[32m'page_59'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'Prompt Engineering\\n- System prompting\\n- Role prompting\\n- Contextual prompting\\n- Step-back \u001b[0m\n",
       "\u001b[32mprompting\\n- Chain of thought\\n- Self consistency\\n- Tree of thoughts\\n- ReAct\\nWe even looked into ways how you \u001b[0m\n",
       "\u001b[32mcan automate your prompts.\\nThe whitepaper then discusses the challenges of gen AI like the problems that can \u001b[0m\n",
       "\u001b[32mhappen when your prompts are insufficient. We closed with best practices on how to become a better prompt \u001b[0m\n",
       "\u001b[32mengineer.'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'image_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'page_59'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Prompt Engineering System prompting Role prompting Contextual prompting Stepback prompting Chain </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of thought Self consistency Tree of thoughts ReAct\\nWe even looked into ways how you can automate your </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompts.\\nThe whitepaper then discusses the challenges of gen AI like the problems that can happen when your </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompts are insufficient. We closed with best practices on how to become a better prompt engineer.'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'image_name'\u001b[0m: \u001b[32m'page_59'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'Prompt Engineering System prompting Role prompting Contextual prompting Stepback prompting Chain \u001b[0m\n",
       "\u001b[32mof thought Self consistency Tree of thoughts ReAct\\nWe even looked into ways how you can automate your \u001b[0m\n",
       "\u001b[32mprompts.\\nThe whitepaper then discusses the challenges of gen AI like the problems that can happen when your \u001b[0m\n",
       "\u001b[32mprompts are insufficient. We closed with best practices on how to become a better prompt engineer.'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = documents[58]\n",
    "print(doc)\n",
    "\n",
    "new_content = re.sub(r'\\s{2,}', ' ', doc.page_content.replace('-', ''))\n",
    "new = Document(metadata=doc.metadata, page_content=new_content)\n",
    "\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744ab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[58] = new\n",
    "print(documents[58])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d5a6e",
   "metadata": {},
   "source": [
    "#### Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "795600d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elasticsearch 연결 (~05/28 Free-Trial)\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts=os.getenv(\"ELASTICSEARCH_URL\"),\n",
    "    api_key=os.getenv(\"ELASTIC_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc32ef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(es.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa7726",
   "metadata": {},
   "source": [
    "#### Vector Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "78515d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ef6fe0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document + 임베딩 저장\n",
    "es_store = ElasticsearchStore.from_documents(\n",
    "    documents=documents,\n",
    "    es_connection=es,\n",
    "    index_name=\"llm-elastic\",\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "896220ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The Gemini temperature control can be understood in a similar way to the softmax function used in machine learning.\n",
       "A low temperature setting mirrors a low softmax temperature <span style=\"font-weight: bold\">(</span>T<span style=\"font-weight: bold\">)</span>, emphasizing a single, preferred temperature with \n",
       "high certainty. A higher Gemini temperature setting is like a high softmax temperature, making a wider range of \n",
       "temperatures around the selected setting more acceptable. This increased uncertainty accommodates scenarios where a\n",
       "rigid, precise temperature may not be essential like for example when experimenting with creative outputs.\n",
       "Top-K and top-P\n",
       "Top-K and top-P <span style=\"font-weight: bold\">(</span>also known as nucleus sampling<span style=\"font-weight: bold\">)</span> are two sampling settings used in LLMs to restrict the predicted \n",
       "next token to come from tokens with the top predicted probabilities. Like temperature, these sampling settings \n",
       "control the randomness and diversity of generated text.\n",
       "- Top-K sampling selects the top K most likely tokens from the model’s predicted distribution. The higher top-K, \n",
       "the more creative and varied the model’s output; the lower top-K, the more restive and factual the model’s output. \n",
       "A top-K of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> is equivalent to greedy decoding.\n",
       "- Top-P sampling selects the top tokens whose cumulative probability does not exceed a certain value <span style=\"font-weight: bold\">(</span>P<span style=\"font-weight: bold\">)</span>. Values \n",
       "for P range from <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> <span style=\"font-weight: bold\">(</span>greedy decoding<span style=\"font-weight: bold\">)</span> to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> <span style=\"font-weight: bold\">(</span>all tokens in the LLM’s vocabulary<span style=\"font-weight: bold\">)</span>.\n",
       "The best way to choose between top-K and top-P is to experiment with both methods <span style=\"font-weight: bold\">(</span>or both together<span style=\"font-weight: bold\">)</span> and see which \n",
       "one produces the results you are looking for.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The Gemini temperature control can be understood in a similar way to the softmax function used in machine learning.\n",
       "A low temperature setting mirrors a low softmax temperature \u001b[1m(\u001b[0mT\u001b[1m)\u001b[0m, emphasizing a single, preferred temperature with \n",
       "high certainty. A higher Gemini temperature setting is like a high softmax temperature, making a wider range of \n",
       "temperatures around the selected setting more acceptable. This increased uncertainty accommodates scenarios where a\n",
       "rigid, precise temperature may not be essential like for example when experimenting with creative outputs.\n",
       "Top-K and top-P\n",
       "Top-K and top-P \u001b[1m(\u001b[0malso known as nucleus sampling\u001b[1m)\u001b[0m are two sampling settings used in LLMs to restrict the predicted \n",
       "next token to come from tokens with the top predicted probabilities. Like temperature, these sampling settings \n",
       "control the randomness and diversity of generated text.\n",
       "- Top-K sampling selects the top K most likely tokens from the model’s predicted distribution. The higher top-K, \n",
       "the more creative and varied the model’s output; the lower top-K, the more restive and factual the model’s output. \n",
       "A top-K of \u001b[1;36m1\u001b[0m is equivalent to greedy decoding.\n",
       "- Top-P sampling selects the top tokens whose cumulative probability does not exceed a certain value \u001b[1m(\u001b[0mP\u001b[1m)\u001b[0m. Values \n",
       "for P range from \u001b[1;36m0\u001b[0m \u001b[1m(\u001b[0mgreedy decoding\u001b[1m)\u001b[0m to \u001b[1;36m1\u001b[0m \u001b[1m(\u001b[0mall tokens in the LLM’s vocabulary\u001b[1m)\u001b[0m.\n",
       "The best way to choose between top-K and top-P is to experiment with both methods \u001b[1m(\u001b[0mor both together\u001b[1m)\u001b[0m and see which \n",
       "one produces the results you are looking for.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 검색 테스트\n",
    "results = es_store.similarity_search(\"What is top k?\", k=1)\n",
    "for r in results:\n",
    "    print(r.page_content)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "088b88cf",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def similarity_search(question):\n",
    "    result = es_store.similarity_search(question, k=3)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71bf6272",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def similarity_search_with_scroe(question):\n",
    "    result = es_store.similarity_search_with_score(question, k=3)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e2a06b",
   "metadata": {},
   "source": [
    "#### Question & Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9274be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f69503f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "You will be provided with an input prompt and content as context that can be used to reply to the prompt.\n",
    "    \n",
    "You will do 2 things:\n",
    "    \n",
    "1. First, you will internally assess whether the content provided is relevant to reply to the input prompt.     \n",
    "2a. If that is the case, answer directly using this content. If the content is relevant, use elements found in the content to craft a reply to the input prompt.\n",
    "2b. If the content is not relevant, use your own knowledge to reply or say that you don't know how to respond if your knowledge is not sufficient to answer.\n",
    "    \n",
    "Stay concise with your answer, replying specifically to the input prompt without mentioning additional information provided in the context content.\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"Content:\\n{content}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "# 벡터 검색 + GPT 답변\n",
    "def answer_question(question):\n",
    "    retriever = es_store.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\"score_threshold\": 0.5, \"k\": 3})\n",
    "    \n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    content = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    messages = {\n",
    "        \"content\" : content,\n",
    "        \"question\" : question\n",
    "    }\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke(messages)\n",
    "    answer = f\"Answer:\\n{response}\\n\\n------\\nRelated content:\\n{content}\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "15aa64a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Answer:\n",
       "Contextual prompting provides specific details or background information pertinent to the current conversation or \n",
       "task. It assists the language model in grasping the nuances of what's being asked, enabling it to tailor its \n",
       "response appropriately. This type of prompting is highly specific to the task or input at hand, helping ensure that\n",
       "responses are relevant and accurate.\n",
       "\n",
       "------\n",
       "Related content:\n",
       "System, contextual and role prompting\n",
       "System, contextual and role prompting are all techniques used to guide how LLMs generate text, but they focus on \n",
       "different aspects:\n",
       " System prompting sets the overall context and purpose for the language model. It defines the ‘big picture’ of what\n",
       "the model should be doing, like translating a language, classifying a review etc.\n",
       " Contextual prompting provides specific details or background information relevant to the current conversation or \n",
       "task. It helps the model to understand the nuances of what’s being asked and tailor the response accordingly.\n",
       " Role prompting assigns a specific character or identity for the language model to adopt. This helps the model \n",
       "generate responses that are consistent with the assigned role and its associated knowledge and behavior.\n",
       "There can be considerable overlap between system, contextual, and role prompting. E.g. a prompt that assigns a role\n",
       "to the system, can also have a context.\n",
       "However, each type of prompt serves a slightly different primary purpose:\n",
       " System prompt: Defines the model’s fundamental capabilities and overarching purpose.\n",
       " Contextual prompt: Provides immediate, taskspecific information to guide the response. It’s highly specific to the\n",
       "current task or input, which is dynamic.\n",
       " Role prompt: Frames the model’s output style and voice. It adds a layer of specificity and personality.\n",
       "\n",
       "Prompt Engineering System prompting Role prompting Contextual prompting Stepback prompting Chain of thought Self \n",
       "consistency Tree of thoughts ReAct\n",
       "We even looked into ways how you can automate your prompts.\n",
       "The whitepaper then discusses the challenges of gen AI like the problems that can happen when your prompts are \n",
       "insufficient. We closed with best practices on how to become a better prompt engineer.\n",
       "\n",
       "Step-back prompting\n",
       "Step-back prompting is a technique for improving the performance by prompting the LLM to first consider a general \n",
       "question related to the specific task at hand, and then feeding the answer to that general question into a \n",
       "subsequent prompt for the specific task. This ‘step back’ allows the LLM to activate relevant background knowledge \n",
       "and reasoning processes before attempting to solve the specific problem.\n",
       "By considering the broader and underlying principles, LLMs can generate more accurate and insightful responses. \n",
       "Step-back prompting encourages LLMs to think critically and apply their knowledge in new and creative ways. It \n",
       "changes the final prompt doing the task by utilizing more knowledge in the LLM’s parameters than would otherwise \n",
       "come into play when the LLM is prompted directly.\n",
       "It can help to mitigate biases in LLM responses, by focusing on general principles instead of specific details, \n",
       "step-back prompting.\n",
       "Let’s have a look into these examples to understand how step-back prompting can improve the results. Let’s first \n",
       "review a traditional prompt <span style=\"font-weight: bold\">(</span>Table <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">)</span> before we compare it to a step back prompt <span style=\"font-weight: bold\">(</span>Table <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">)</span>:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Answer:\n",
       "Contextual prompting provides specific details or background information pertinent to the current conversation or \n",
       "task. It assists the language model in grasping the nuances of what's being asked, enabling it to tailor its \n",
       "response appropriately. This type of prompting is highly specific to the task or input at hand, helping ensure that\n",
       "responses are relevant and accurate.\n",
       "\n",
       "------\n",
       "Related content:\n",
       "System, contextual and role prompting\n",
       "System, contextual and role prompting are all techniques used to guide how LLMs generate text, but they focus on \n",
       "different aspects:\n",
       " System prompting sets the overall context and purpose for the language model. It defines the ‘big picture’ of what\n",
       "the model should be doing, like translating a language, classifying a review etc.\n",
       " Contextual prompting provides specific details or background information relevant to the current conversation or \n",
       "task. It helps the model to understand the nuances of what’s being asked and tailor the response accordingly.\n",
       " Role prompting assigns a specific character or identity for the language model to adopt. This helps the model \n",
       "generate responses that are consistent with the assigned role and its associated knowledge and behavior.\n",
       "There can be considerable overlap between system, contextual, and role prompting. E.g. a prompt that assigns a role\n",
       "to the system, can also have a context.\n",
       "However, each type of prompt serves a slightly different primary purpose:\n",
       " System prompt: Defines the model’s fundamental capabilities and overarching purpose.\n",
       " Contextual prompt: Provides immediate, taskspecific information to guide the response. It’s highly specific to the\n",
       "current task or input, which is dynamic.\n",
       " Role prompt: Frames the model’s output style and voice. It adds a layer of specificity and personality.\n",
       "\n",
       "Prompt Engineering System prompting Role prompting Contextual prompting Stepback prompting Chain of thought Self \n",
       "consistency Tree of thoughts ReAct\n",
       "We even looked into ways how you can automate your prompts.\n",
       "The whitepaper then discusses the challenges of gen AI like the problems that can happen when your prompts are \n",
       "insufficient. We closed with best practices on how to become a better prompt engineer.\n",
       "\n",
       "Step-back prompting\n",
       "Step-back prompting is a technique for improving the performance by prompting the LLM to first consider a general \n",
       "question related to the specific task at hand, and then feeding the answer to that general question into a \n",
       "subsequent prompt for the specific task. This ‘step back’ allows the LLM to activate relevant background knowledge \n",
       "and reasoning processes before attempting to solve the specific problem.\n",
       "By considering the broader and underlying principles, LLMs can generate more accurate and insightful responses. \n",
       "Step-back prompting encourages LLMs to think critically and apply their knowledge in new and creative ways. It \n",
       "changes the final prompt doing the task by utilizing more knowledge in the LLM’s parameters than would otherwise \n",
       "come into play when the LLM is prompted directly.\n",
       "It can help to mitigate biases in LLM responses, by focusing on general principles instead of specific details, \n",
       "step-back prompting.\n",
       "Let’s have a look into these examples to understand how step-back prompting can improve the results. Let’s first \n",
       "review a traditional prompt \u001b[1m(\u001b[0mTable \u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m before we compare it to a step back prompt \u001b[1m(\u001b[0mTable \u001b[1;36m9\u001b[0m\u001b[1m)\u001b[0m:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_question = \"explain about contextual prompting\"\n",
    "answer = answer_question(user_question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
